# Add exploding gradients
# What is exploding gradient?
Exploding gradients is a problem that can occur during the training of deep neural networks, particularly in recurrent neural networks (RNNs). It happens when the gradients used to update the weights during backpropagation become very large, which leads to numerical instability and can cause the weights to update in unpredictable ways.Understanding and mitigating the issue of exploding gradients is an essential aspect of training deep neural networks successfully.

## Visit the following resources:
- [Andrew Ng explains the exploding gradient problem and how it can be addressed.](https://www.youtube.com/watch?v=1waHlpKiNyY)

- [This blog post provides a clear explanation of the exploding gradient problem and discusses some techniques to prevent it](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)

- [This article explains the causes of exploding gradients and offers solutions for preventing them.](https://towardsdatascience.com/how-to-fix-exploding-gradients-in-deep-neural-networks-7c9f3ce99f5)
