# Variational Autoencoder with Attention resource 
Variational autoencoders with attention mechanisms have become a popular choice for unsupervised learning tasks. Variational autoencoders use a probabilistic approach to learn a latent representation of the input data, while attention mechanisms allow the model to focus on specific parts of the input. By combining these two techniques, variational autoencoders with attention can generate more accurate and diverse samples.

One popular type of attention mechanism used in variational autoencoders is self-attention. Self-attention allows the model to attend to different parts of the input sequence, based on their relevance to the current output. This helps the model to capture long-range dependencies and improve the quality of generated samples.

# valuable resource
- [An introduction to variational autoencoders (VAE)](https://www.youtube.com/watch?v=YV9D3TWY5Zo)
- [Variational Auto Encoder (VAE) - Theory](https://www.youtube.com/watch?v=vJo7hiMxbQ8)
- [bayesian-machine-learning](https://github.com/krasserm/bayesian-machine-learning)
- [intuitively-understanding-variational-autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)
- [Structural Attention-Based Recurrent Variational Autoencoder](https://arxiv.org/pdf/2301.03634.pdf)
