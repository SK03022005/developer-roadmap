 # DistilBERT :


 DistilBERT is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model, which is a type of deep learning model based on the transformer architecture. BERT is known for its state-of-the-art performance on a variety of natural language processing (NLP) tasks, including text classification, question answering, and language translation. the knowledge learned by a larger BERT model into a smaller DistilBERT model using a process called distillation.

# DistilBERT resource :

- [DistilBERT Overview](https://huggingface.co/docs/transformers/model_doc/distilbert)


- [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python)


- [Welcome fastai to the Hugging Face Hub](https://huggingface.co/blog/fastai)


- [Hyperparameter Search with Transformers and Ray Tune](https://huggingface.co/blog/ray-tune)


- [Fine Tuning DistilBERT for MultiLabel Text Classification](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)






